{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing packages and loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import logging\n",
    "import argparse\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from time import time \n",
    "from tqdm import tqdm \n",
    "from glob import glob\n",
    "from os.path import dirname as up\n",
    "from rasterio.enums import Resampling\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "from timm.utils import ModelEma\n",
    "\n",
    "from segmentation_models_pytorch.decoders.unet.model import Unet\n",
    "from vscp import VSCP\n",
    "from test_time_aug import TTA\n",
    "from metrics import Evaluation, confusion_matrix\n",
    "from assets import bool_flag, cosine_scheduler, labels, mados_cat_mapping, mados_color_mapping\n",
    "#from dataset import MADOS, gen_weights, class_distr, bands_mean, bands_std\n",
    "from dataset_crop import MADOS, gen_weights, class_distr, bands_mean, bands_std\n",
    "import torch.nn as nn\n",
    "import pickle\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_directory = os.path.join(\"/home/ubuntu/Tesi/Late-Fusion-Unet\", 'log')\n",
    "time_now = str(int(time() / 60))\n",
    "log_file = os.path.join(log_directory, 'log_marinext_' + time_now + '.log')\n",
    "\n",
    "logging.basicConfig(filename=log_file, filemode='a', level=logging.INFO, format='%(name)s - %(levelname)s - %(message)s')\n",
    "logging.info('*' * 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    # Pytorch Reproducibility\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "def seed_worker(worker_id):\n",
    "    # DataLoader Workers Reproducibility\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-23 15:55:18.965141: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-23 15:55:20.241277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "seed_all(0)\n",
    "g=torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "writer = SummaryWriter(os.path.join(log_directory, 'logs', 'tsboard_segm'+'_'+time_now))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits_path = os.path.join(\"/home/ubuntu/Tesi/MADOS_crop/MADOS\",'splits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load train set to memory:   0%|          | 0/175 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/rasterio/__init__.py:304: NotGeoreferencedWarning: Dataset has no geotransform, gcps, or rpcs. The identity matrix will be returned.\n",
      "  dataset = DatasetReader(path, driver=driver, sharing=sharing, **kwargs)\n",
      "Load train set to memory: 100%|██████████| 175/175 [02:17<00:00,  1.27it/s]\n",
      "Load val set to memory: 100%|██████████| 175/175 [01:12<00:00,  2.40it/s]\n",
      "Load test set to memory: 100%|██████████| 175/175 [01:12<00:00,  2.41it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_train = MADOS(\"/home/ubuntu/Tesi/MADOS_crop/MADOS\", splits_path, 'train')\n",
    "dataset_val = MADOS(\"/home/ubuntu/Tesi/MADOS_crop/MADOS\", splits_path, 'val')\n",
    "dataset_test = MADOS(\"/home/ubuntu/Tesi/MADOS_crop/MADOS\", splits_path, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset_train, \n",
    "                          batch_size = 5, \n",
    "                          shuffle = True,\n",
    "                          num_workers = 0,           # 0 is the main process\n",
    "                          pin_memory = False,        # Use pinned memory or not\n",
    "                          prefetch_factor = 2,      # Number of sample loaded in advance by each worker\n",
    "                          persistent_workers= False, # This allows to maintain the workers Dataset instances alive.\n",
    "                          worker_init_fn=seed_worker,\n",
    "                          generator=g,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(dataset_val, \n",
    "                          batch_size = 5, \n",
    "                          shuffle = False,\n",
    "                          num_workers = 0,           # 0 is the main process\n",
    "                          pin_memory = False,        # Use pinned memory or not\n",
    "                          prefetch_factor = 2,      # Number of sample loaded in advance by each worker\n",
    "                          persistent_workers= False, # This allows to maintain the workers Dataset instances alive.\n",
    "                          worker_init_fn=seed_worker,\n",
    "                          generator=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(dataset_test, \n",
    "                          batch_size = 1, \n",
    "                          shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU presence check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use gpu or cpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unet(\n",
       "  (encoder10): ResNetEncoder(\n",
       "    (conv1): Conv2d(4, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder20): ResNetEncoder(\n",
       "    (conv1): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (encoder60): ResNetEncoder(\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): UnetDecoder(\n",
       "    (center): Identity()\n",
       "    (blocks): ModuleList(\n",
       "      (0): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(1792, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (1): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(384, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (2): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(192, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (3): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "      (4): DecoderBlock(\n",
       "        (conv1): Conv2dReLU(\n",
       "          (0): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention1): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "        (conv2): Conv2dReLU(\n",
       "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "        )\n",
       "        (attention2): Attention(\n",
       "          (attention): Identity()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(16, 15, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Unet('resnet18', in_channels10=4, in_channels20=6, in_channels60=1, classes=15)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]          12,544\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "    ResNetEncoder-67  [[-1, 4, 224, 224], [-1, 64, 112, 112], [-1, 64, 56, 56], [-1, 128, 28, 28], [-1, 256, 14, 14], [-1, 512, 7, 7]]               0\n",
      "           Conv2d-68         [-1, 64, 112, 112]          18,816\n",
      "      BatchNorm2d-69         [-1, 64, 112, 112]             128\n",
      "             ReLU-70         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-71           [-1, 64, 56, 56]               0\n",
      "           Conv2d-72           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-73           [-1, 64, 56, 56]             128\n",
      "             ReLU-74           [-1, 64, 56, 56]               0\n",
      "           Conv2d-75           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-76           [-1, 64, 56, 56]             128\n",
      "             ReLU-77           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-78           [-1, 64, 56, 56]               0\n",
      "           Conv2d-79           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-80           [-1, 64, 56, 56]             128\n",
      "             ReLU-81           [-1, 64, 56, 56]               0\n",
      "           Conv2d-82           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-83           [-1, 64, 56, 56]             128\n",
      "             ReLU-84           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-85           [-1, 64, 56, 56]               0\n",
      "           Conv2d-86          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-87          [-1, 128, 28, 28]             256\n",
      "             ReLU-88          [-1, 128, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "           Conv2d-91          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-92          [-1, 128, 28, 28]             256\n",
      "             ReLU-93          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-96          [-1, 128, 28, 28]             256\n",
      "             ReLU-97          [-1, 128, 28, 28]               0\n",
      "           Conv2d-98          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-99          [-1, 128, 28, 28]             256\n",
      "            ReLU-100          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-103          [-1, 256, 14, 14]             512\n",
      "            ReLU-104          [-1, 256, 14, 14]               0\n",
      "          Conv2d-105          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-106          [-1, 256, 14, 14]             512\n",
      "          Conv2d-107          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-108          [-1, 256, 14, 14]             512\n",
      "            ReLU-109          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-110          [-1, 256, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [-1, 256, 14, 14]             512\n",
      "            ReLU-116          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-117          [-1, 256, 14, 14]               0\n",
      "          Conv2d-118            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-119            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-120            [-1, 512, 7, 7]               0\n",
      "          Conv2d-121            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-122            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-123            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-124            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-125            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-126            [-1, 512, 7, 7]               0\n",
      "          Conv2d-127            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-128            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-129            [-1, 512, 7, 7]               0\n",
      "          Conv2d-130            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-131            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-132            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-133            [-1, 512, 7, 7]               0\n",
      "   ResNetEncoder-134  [[-1, 6, 224, 224], [-1, 64, 112, 112], [-1, 64, 56, 56], [-1, 128, 28, 28], [-1, 256, 14, 14], [-1, 512, 7, 7]]               0\n",
      "          Conv2d-135         [-1, 64, 112, 112]           3,136\n",
      "     BatchNorm2d-136         [-1, 64, 112, 112]             128\n",
      "            ReLU-137         [-1, 64, 112, 112]               0\n",
      "       MaxPool2d-138           [-1, 64, 56, 56]               0\n",
      "          Conv2d-139           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-140           [-1, 64, 56, 56]             128\n",
      "            ReLU-141           [-1, 64, 56, 56]               0\n",
      "          Conv2d-142           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-143           [-1, 64, 56, 56]             128\n",
      "            ReLU-144           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-145           [-1, 64, 56, 56]               0\n",
      "          Conv2d-146           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-147           [-1, 64, 56, 56]             128\n",
      "            ReLU-148           [-1, 64, 56, 56]               0\n",
      "          Conv2d-149           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-150           [-1, 64, 56, 56]             128\n",
      "            ReLU-151           [-1, 64, 56, 56]               0\n",
      "      BasicBlock-152           [-1, 64, 56, 56]               0\n",
      "          Conv2d-153          [-1, 128, 28, 28]          73,728\n",
      "     BatchNorm2d-154          [-1, 128, 28, 28]             256\n",
      "            ReLU-155          [-1, 128, 28, 28]               0\n",
      "          Conv2d-156          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-157          [-1, 128, 28, 28]             256\n",
      "          Conv2d-158          [-1, 128, 28, 28]           8,192\n",
      "     BatchNorm2d-159          [-1, 128, 28, 28]             256\n",
      "            ReLU-160          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-161          [-1, 128, 28, 28]               0\n",
      "          Conv2d-162          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-163          [-1, 128, 28, 28]             256\n",
      "            ReLU-164          [-1, 128, 28, 28]               0\n",
      "          Conv2d-165          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-166          [-1, 128, 28, 28]             256\n",
      "            ReLU-167          [-1, 128, 28, 28]               0\n",
      "      BasicBlock-168          [-1, 128, 28, 28]               0\n",
      "          Conv2d-169          [-1, 256, 14, 14]         294,912\n",
      "     BatchNorm2d-170          [-1, 256, 14, 14]             512\n",
      "            ReLU-171          [-1, 256, 14, 14]               0\n",
      "          Conv2d-172          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-173          [-1, 256, 14, 14]             512\n",
      "          Conv2d-174          [-1, 256, 14, 14]          32,768\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-177          [-1, 256, 14, 14]               0\n",
      "          Conv2d-178          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-179          [-1, 256, 14, 14]             512\n",
      "            ReLU-180          [-1, 256, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "      BasicBlock-184          [-1, 256, 14, 14]               0\n",
      "          Conv2d-185            [-1, 512, 7, 7]       1,179,648\n",
      "     BatchNorm2d-186            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-187            [-1, 512, 7, 7]               0\n",
      "          Conv2d-188            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-189            [-1, 512, 7, 7]           1,024\n",
      "          Conv2d-190            [-1, 512, 7, 7]         131,072\n",
      "     BatchNorm2d-191            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-192            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-193            [-1, 512, 7, 7]               0\n",
      "          Conv2d-194            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-195            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-196            [-1, 512, 7, 7]               0\n",
      "          Conv2d-197            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-198            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-199            [-1, 512, 7, 7]               0\n",
      "      BasicBlock-200            [-1, 512, 7, 7]               0\n",
      "   ResNetEncoder-201  [[-1, 1, 224, 224], [-1, 64, 112, 112], [-1, 64, 56, 56], [-1, 128, 28, 28], [-1, 256, 14, 14], [-1, 512, 7, 7]]               0\n",
      "        Identity-202           [-1, 1536, 7, 7]               0\n",
      "        Identity-203         [-1, 1792, 14, 14]               0\n",
      "       Attention-204         [-1, 1792, 14, 14]               0\n",
      "          Conv2d-205          [-1, 256, 14, 14]       4,128,768\n",
      "     BatchNorm2d-206          [-1, 256, 14, 14]             512\n",
      "            ReLU-207          [-1, 256, 14, 14]               0\n",
      "          Conv2d-208          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-209          [-1, 256, 14, 14]             512\n",
      "            ReLU-210          [-1, 256, 14, 14]               0\n",
      "        Identity-211          [-1, 256, 14, 14]               0\n",
      "       Attention-212          [-1, 256, 14, 14]               0\n",
      "    DecoderBlock-213          [-1, 256, 14, 14]               0\n",
      "        Identity-214          [-1, 384, 28, 28]               0\n",
      "       Attention-215          [-1, 384, 28, 28]               0\n",
      "          Conv2d-216          [-1, 128, 28, 28]         442,368\n",
      "     BatchNorm2d-217          [-1, 128, 28, 28]             256\n",
      "            ReLU-218          [-1, 128, 28, 28]               0\n",
      "          Conv2d-219          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-220          [-1, 128, 28, 28]             256\n",
      "            ReLU-221          [-1, 128, 28, 28]               0\n",
      "        Identity-222          [-1, 128, 28, 28]               0\n",
      "       Attention-223          [-1, 128, 28, 28]               0\n",
      "    DecoderBlock-224          [-1, 128, 28, 28]               0\n",
      "        Identity-225          [-1, 192, 56, 56]               0\n",
      "       Attention-226          [-1, 192, 56, 56]               0\n",
      "          Conv2d-227           [-1, 64, 56, 56]         110,592\n",
      "     BatchNorm2d-228           [-1, 64, 56, 56]             128\n",
      "            ReLU-229           [-1, 64, 56, 56]               0\n",
      "          Conv2d-230           [-1, 64, 56, 56]          36,864\n",
      "     BatchNorm2d-231           [-1, 64, 56, 56]             128\n",
      "            ReLU-232           [-1, 64, 56, 56]               0\n",
      "        Identity-233           [-1, 64, 56, 56]               0\n",
      "       Attention-234           [-1, 64, 56, 56]               0\n",
      "    DecoderBlock-235           [-1, 64, 56, 56]               0\n",
      "        Identity-236        [-1, 128, 112, 112]               0\n",
      "       Attention-237        [-1, 128, 112, 112]               0\n",
      "          Conv2d-238         [-1, 32, 112, 112]          36,864\n",
      "     BatchNorm2d-239         [-1, 32, 112, 112]              64\n",
      "            ReLU-240         [-1, 32, 112, 112]               0\n",
      "          Conv2d-241         [-1, 32, 112, 112]           9,216\n",
      "     BatchNorm2d-242         [-1, 32, 112, 112]              64\n",
      "            ReLU-243         [-1, 32, 112, 112]               0\n",
      "        Identity-244         [-1, 32, 112, 112]               0\n",
      "       Attention-245         [-1, 32, 112, 112]               0\n",
      "    DecoderBlock-246         [-1, 32, 112, 112]               0\n",
      "          Conv2d-247         [-1, 16, 224, 224]           4,608\n",
      "     BatchNorm2d-248         [-1, 16, 224, 224]              32\n",
      "            ReLU-249         [-1, 16, 224, 224]               0\n",
      "          Conv2d-250         [-1, 16, 224, 224]           2,304\n",
      "     BatchNorm2d-251         [-1, 16, 224, 224]              32\n",
      "            ReLU-252         [-1, 16, 224, 224]               0\n",
      "        Identity-253         [-1, 16, 224, 224]               0\n",
      "       Attention-254         [-1, 16, 224, 224]               0\n",
      "    DecoderBlock-255         [-1, 16, 224, 224]               0\n",
      "     UnetDecoder-256         [-1, 16, 224, 224]               0\n",
      "          Conv2d-257         [-1, 15, 224, 224]           2,175\n",
      "        Identity-258         [-1, 15, 224, 224]               0\n",
      "        Identity-259         [-1, 15, 224, 224]               0\n",
      "      Activation-260         [-1, 15, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 39,048,831\n",
      "Trainable params: 39,048,831\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 11565367296.00\n",
      "Forward/backward pass size (MB): 368.46\n",
      "Params size (MB): 148.96\n",
      "Estimated Total Size (MB): 11565367813.42\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, [(4, 224, 224), (6, 224, 224), (1, 224, 224)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMA è una forma di media mobile che attribuisce pesi decrescenti ai dati nel tempo, con un peso maggiore ai dati più recenti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ema = None\n",
    "if True:\n",
    "    model_ema = ModelEma(\n",
    "        model,\n",
    "        decay=0.999,\n",
    "        device=device,\n",
    "        resume='')\n",
    "    \n",
    "    ema_decay_schedule = cosine_scheduler(0.999, 0.999, 80, len(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted Cross Entropy Loss & adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = torch.tensor(np.array([64.39, 28.33, 70.90, 75.34, 75.33, 19.25, 5.67, 26.99, 53.84, 23.63, 15.65, 75.48, 75.03, 76.19, 20.38])).float()\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction= 'none', weight=weight.to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0002, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr_on_plateau=0\n",
    "if reduce_lr_on_plateau==1:\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "else:\n",
    "    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, '[45,65]', gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 80\n",
    "eval_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/286 [00:00<?, ?it/s]/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3509: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/torch/nn/functional.py:3631: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.76it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.73it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.72it/s]\n",
      "training: 100%|██████████| 286/286 [02:39<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.74it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.94it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.74it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.75it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.90it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.86it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.75it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.91it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.91it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.84it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.93it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.90it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.87it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.84it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.93it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.87it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.76it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.75it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.87it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.90it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.86it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.93it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.67it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.75it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.91it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.90it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.76it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.73it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.75it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.88it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.93it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.93it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.86it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.83it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.82it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.89it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "training: 100%|██████████| 286/286 [02:38<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.76it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.85it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.83it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.83it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.80it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.79it/s]\n",
      "training: 100%|██████████| 286/286 [02:37<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.77it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.87it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.82it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:19<00:00,  6.78it/s]\n",
      "training: 100%|██████████| 286/286 [02:36<00:00,  1.83it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.81it/s]\n",
      "validating: 100%|██████████| 129/129 [00:18<00:00,  6.92it/s]\n"
     ]
    }
   ],
   "source": [
    "#Train\n",
    "model.train()\n",
    "\n",
    "best_val_loss = 100\n",
    "best_val_loss_ema = 100\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    training_loss = []\n",
    "    training_batches = 0\n",
    "    \n",
    "    i_board = 0\n",
    "    for it, (image10, image20, image60, target) in enumerate(tqdm(train_loader, desc=\"training\")):\n",
    "\n",
    "        it = len(train_loader) * (epoch-1) + it  # global training iteration\n",
    "\n",
    "        #VSCP\n",
    "        image_augmented10, image_augmented20, image_augmented60, target_augmented = VSCP(\n",
    "            image10.cpu().detach().numpy(), \n",
    "            image20.cpu().detach().numpy(), \n",
    "            image60.cpu().detach().numpy(), \n",
    "            target.cpu().detach().numpy())\n",
    "        image10 = torch.cat([image10, torch.tensor(image_augmented10).to(image10.device)])\n",
    "        image20 = torch.cat([image20, torch.tensor(image_augmented20).to(image20.device)])\n",
    "        image60 = torch.cat([image60, torch.tensor(image_augmented60).to(image60.device)])\n",
    "        target = torch.cat([target, torch.tensor(target_augmented).to(target.device)])\n",
    "\n",
    "        image10 = image10.to(device)\n",
    "        image20 = image20.to(device)\n",
    "        image60 = image60.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        torch.use_deterministic_algorithms(False)\n",
    "        inp = image10, image20, image60\n",
    "        logits = F.upsample(input=model(image10, image20, image60), size=image10.size()[2:4], mode='bilinear')\n",
    "        logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "        logits = logits.reshape((-1,15))\n",
    "        target = target.reshape(-1)\n",
    "        mask = target != -1\n",
    "        logits = logits[mask]\n",
    "        target = target[mask]\n",
    "        loss = criterion(logits, target)\n",
    "        #loss = loss.mean()\n",
    "        loss = loss.sum() / weight[target].sum()\n",
    "        loss.backward()\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "\n",
    "        training_batches += target.shape[0]\n",
    "\n",
    "        training_loss.append((loss.data*target.shape[0]).tolist()) \n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), float('inf'))\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        model_ema.decay = ema_decay_schedule[it]\n",
    "        model_ema.update(model)\n",
    "        \n",
    "        # Write running loss\n",
    "        writer.add_scalar('training loss', loss , (epoch - 1) * len(train_loader)+i_board)\n",
    "        i_board+=1\n",
    "\n",
    "    logging.info(\"Training loss was: \" + str(sum(training_loss) / training_batches))\n",
    "    \n",
    "    logging.info(\"Saving models\")\n",
    "    model_dir = os.path.join(up(os.path.abspath(\"/home/ubuntu/Tesi/Late-Fusion-Unet\")), 'trained_models_LF_Unet')\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    #torch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n",
    "    \n",
    "    # Start Evaluation\n",
    "    if epoch % eval_every == 0 or epoch==1:\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = []\n",
    "        val_batches = 0\n",
    "        y_true_val = []\n",
    "        y_predicted_val = []\n",
    "        \n",
    "        seed_all(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (image10, image20, image60, target) in tqdm(val_loader, desc=\"validating\"):\n",
    "\n",
    "                image10 = image10.to(device)\n",
    "                image20 = image20.to(device)\n",
    "                image60 = image60.to(device)\n",
    "                target = target.long().to(device)\n",
    "                \n",
    "                logits = F.upsample(input=model(image10, image20, image60), size=(target.shape[-2], target.shape[-1]), mode='bilinear')\n",
    "                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "                logits = logits.reshape((-1,15))\n",
    "                target = target.reshape(-1)\n",
    "                mask = target != -1\n",
    "                logits = logits[mask]\n",
    "                target = target[mask]\n",
    "                loss = criterion(logits, target)\n",
    "                #loss = loss.mean()\n",
    "                loss = loss.sum() / weight[target].sum()\n",
    "                \n",
    "                probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "                target = target.cpu().numpy()\n",
    "                \n",
    "                val_batches += target.shape[0]\n",
    "                val_loss.append((loss.data*target.shape[0]).tolist())\n",
    "                y_predicted_val += probs.argmax(1).tolist()\n",
    "                y_true_val += target.tolist()\n",
    "                    \n",
    "                \n",
    "            y_predicted_val = np.asarray(y_predicted_val)\n",
    "            y_true_val = np.asarray(y_true_val)\n",
    "            #acc_val = Evaluation(y_predicted_val, y_true_val)\n",
    "        \n",
    "        val_loss_mean = sum(val_loss) / val_batches\n",
    "\n",
    "        # Check if the current validation loss is the best encountered so far\n",
    "        if val_loss_mean < best_val_loss:\n",
    "            best_val_loss = val_loss_mean\n",
    "            logging.info(\"Found new best validation loss at epoch {}: {}\".format(epoch, best_val_loss))\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, 'model.pth'))\n",
    "\n",
    "        # Save Scores    \n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\"Evaluating model..\")\n",
    "        logging.info(\"Val loss was: \" + str(sum(val_loss) / val_batches))\n",
    "        logging.info(\"RESULTS AFTER EPOCH \" +str(epoch) + \": \\n\")\n",
    "        #logging.info(\"Evaluation: \" + str(acc_val))\n",
    "    \n",
    "        writer.add_scalars('Loss per epoch', {'Val loss':sum(val_loss) / val_batches, \n",
    "                                                'Train loss':sum(training_loss) / training_batches}, epoch)\n",
    "        \n",
    "        #writer.add_scalar('Precision/val macroPrec', acc_val[\"macroPrec\"] , epoch)\n",
    "        #writer.add_scalar('Precision/val microPrec', acc_val[\"microPrec\"] , epoch)\n",
    "        #writer.add_scalar('Precision/val weightPrec', acc_val[\"weightPrec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val macroRec', acc_val[\"macroRec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val microRec', acc_val[\"microRec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val weightRec', acc_val[\"weightRec\"] , epoch)\n",
    "        #writer.add_scalar('F1/val macroF1', acc_val[\"macroF1\"] , epoch)\n",
    "        #writer.add_scalar('F1/val microF1', acc_val[\"microF1\"] , epoch)\n",
    "        #writer.add_scalar('F1/val weightF1', acc_val[\"weightF1\"] , epoch)\n",
    "        #writer.add_scalar('IoU/val MacroIoU', acc_val[\"IoU\"] , epoch)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    # Start EMA Evaluation\n",
    "    if (epoch % eval_every == 0 or epoch==1):\n",
    "        \n",
    "        logging.info(\"Saving models\")\n",
    "        model_dir = os.path.join(up(os.path.abspath(\"/home/ubuntu/Tesi/Late-Fusion-Unet\")), 'trained_models_LF_Unet')\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        #torch.save(model_ema.ema.state_dict(), os.path.join(model_dir, 'model_ema.pth'))\n",
    "\n",
    "        val_loss_ema = []\n",
    "        val_batches_ema = 0\n",
    "        y_true_val_ema = []\n",
    "        y_predicted_val_ema = []\n",
    "                        \n",
    "        seed_all(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for (image10, image20, image60, target) in tqdm(val_loader, desc=\"validating\"):\n",
    "                \n",
    "                image10 = image10.to(device)\n",
    "                image20 = image20.to(device)\n",
    "                image60 = image60.to(device)\n",
    "                target = target.long().to(device)\n",
    "                \n",
    "                logits = model_ema.ema(image10, image20, image60)\n",
    "                logits = F.upsample(input=logits, size=(target.shape[-2], target.shape[-1]), mode='bilinear')\n",
    "                logits = torch.movedim(logits, (0,1,2,3), (0,3,1,2))\n",
    "                logits = logits.reshape((-1,15))\n",
    "                target = target.reshape(-1)\n",
    "                mask = target != -1\n",
    "                logits = logits[mask]\n",
    "                target = target[mask]\n",
    "                loss = criterion(logits, target)\n",
    "                #loss = loss.mean()\n",
    "                loss = loss.sum() / weight[target].sum()\n",
    "                \n",
    "                probs = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "                target = target.cpu().numpy()\n",
    "                \n",
    "                val_batches_ema += target.shape[0]\n",
    "                val_loss_ema.append((loss.data*target.shape[0]).tolist())\n",
    "                y_predicted_val_ema += probs.argmax(1).tolist()\n",
    "                y_true_val_ema += target.tolist()\n",
    "                \n",
    "            y_predicted_val_ema = np.asarray(y_predicted_val_ema)\n",
    "            y_true_val_ema = np.asarray(y_true_val_ema)\n",
    "            #acc_val_ema = Evaluation(y_predicted_val_ema, y_true_val_ema)\n",
    "            \n",
    "        val_loss_mean = sum(val_loss_ema) / val_batches_ema\n",
    "\n",
    "        # Check if the current validation loss is the best encountered so far\n",
    "        if val_loss_mean < best_val_loss_ema:\n",
    "            best_val_loss_ema = val_loss_mean\n",
    "            logging.info(\"Found new best validation loss at epoch {}: {}\".format(epoch, best_val_loss_ema))\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, 'model_ema.pth'))\n",
    "\n",
    "        # Save Scores\n",
    "        logging.info(\"\\n\")\n",
    "        logging.info(\"Evaluating EMA model..\")\n",
    "        logging.info(\"val loss was: \" + str(sum(val_loss_ema) / val_batches_ema))\n",
    "        logging.info(\"RESULTS AFTER EPOCH \" +str(epoch) + \": \\n\")\n",
    "        #logging.info(\"Evaluation: \" + str(acc_val_ema))\n",
    "        \n",
    "        writer.add_scalars('Loss per epoch (EMA)', {'val loss':sum(val_loss_ema) / val_batches_ema}, epoch)\n",
    "        #writer.add_scalar('Precision/val macroPrec (EMA)', acc_val_ema[\"macroPrec\"] , epoch)\n",
    "        #writer.add_scalar('Precision/val microPrec (EMA)', acc_val_ema[\"microPrec\"] , epoch)\n",
    "        #writer.add_scalar('Precision/val weightPrec (EMA)', acc_val_ema[\"weightPrec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val macroRec (EMA)', acc_val_ema[\"macroRec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val microRec (EMA)', acc_val_ema[\"microRec\"] , epoch)\n",
    "        #writer.add_scalar('Recall/val weightRec (EMA)', acc_val_ema[\"weightRec\"] , epoch)\n",
    "        #writer.add_scalar('F1/val macroF1 (EMA)', acc_val_ema[\"macroF1\"] , epoch)\n",
    "        #writer.add_scalar('F1/val microF1 (EMA)', acc_val_ema[\"microF1\"] , epoch)\n",
    "        #writer.add_scalar('F1/val weightF1 (EMA)', acc_val_ema[\"weightF1\"] , epoch)\n",
    "        #writer.add_scalar('IoU/val MacroIoU (EMA)', acc_val_ema[\"IoU\"] , epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_ema.ema.state_dict(), os.path.join(model_dir, 'model_ema_80.pth'))\n",
    "torch.save(model.state_dict(), os.path.join(model_dir, 'model_80.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list2 = []\n",
    "#models_files = glob(os.path.join(os.path.join(\"C:\\\\Users\\\\lucap\\\\OneDrive\\\\Desktop\\\\Tesi\\\\Python Code\\\\trained_models_finito\", '1'),'*.pth'))\n",
    "models_files = glob(os.path.join(os.path.join(\"/home/ubuntu/Tesi/trained_models_LF_Unet\"),'model_ema_80.pth'))\n",
    "#models_files = glob(\"C:\\Users\\lucap\\OneDrive\\Desktop\\Tesi\\trained_models\\10\\model_ema.pth\")\n",
    "\n",
    "for model_file in models_files:\n",
    "\n",
    "    model2 = Unet('resnet18', in_channels10=4, in_channels20=6, in_channels60=1, classes=15)\n",
    "\n",
    "    model2.to(device)\n",
    "\n",
    "    \n",
    "    logging.info('Loading model files from folder: %s' % model_file)\n",
    "\n",
    "    checkpoint = torch.load(model_file, map_location = device)\n",
    "    checkpoint = {k.replace('decode_head', 'decoder'):v for k,v in checkpoint.items() if ('proj1' not in k) and ('proj2' not in k)}\n",
    "\n",
    "    model2.load_state_dict(checkpoint)\n",
    "\n",
    "    del checkpoint  # dereference\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model2.eval()\n",
    "    \n",
    "    models_list2.append(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_predicted = []\n",
    "                            \n",
    "with torch.no_grad():\n",
    "    for i, (image10, image20, image60, target) in enumerate(tqdm(test_loader, desc=\"testing\")):\n",
    "\n",
    "        image10 = TTA(image10)   \n",
    "        image20 = TTA(image20)   \n",
    "        image60 = TTA(image60)            \n",
    "\n",
    "        image10 = image10.to(device)\n",
    "        image20 = image20.to(device)\n",
    "        image60 = image60.to(device)\n",
    "        target = target.long().to(device)\n",
    "\n",
    "        seed_all(0)\n",
    "        \n",
    "        logits = model2(image10, image20, image60)\n",
    "        logits = F.upsample(input=logits, size=(target.shape[-2], target.shape[-1]), mode='bilinear')\n",
    "\n",
    "        # Accuracy metrics only on annotated pixels\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1)\n",
    "        predictions = probs.argmax(1)\n",
    "        \n",
    "        predictions = TTA(predictions, reverse_aggregation = True)\n",
    "            \n",
    "        predictions = predictions.reshape(-1)\n",
    "        target = target[0].reshape(-1)\n",
    "        mask = target != -1\n",
    "        \n",
    "        predictions = predictions[mask].cpu().numpy()\n",
    "        target = target[mask]\n",
    "        \n",
    "        target = target.cpu().numpy()\n",
    "\n",
    "        y_predicted += predictions.tolist()\n",
    "        y_true += target.tolist()\n",
    "\n",
    "    # Save Scores\n",
    "    #acc = Evaluation(y_predicted, y_true)\n",
    "    logging.info(\"\\n\")\n",
    "    #logging.info(\"STATISTICS: \\n\")\n",
    "    #logging.info(\"Evaluation: \" + str(acc))\n",
    "    #print(\"Evaluation: \" + str(acc))\n",
    "    conf_mat = confusion_matrix(y_true, y_predicted, labels, True)\n",
    "    logging.info(\"Confusion Matrix:  \\n\" + str(conf_mat.to_string()))\n",
    "    print(\"Confusion Matrix:  \\n\" + str(conf_mat.to_string()))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
